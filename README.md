# Face-Controlled-User-Interface
Takes inputs from face of the user to interact with user interface on computer. Uses movement of nose to control cursor, left eye winking to left click, right eye winking to right click, and open mouth to bring up and down keyboard. Uses MediaPipe to capture facial positions as inputs, uses PyAutoGUI to automate mouse and keyboard functions based on detected motions. Stores user facial landmarks data on SQLite database for future use in training machine learning models. Trains machine learning models using random forest with scikit-learn to adapt to facial landmarks of each user. Stores trained machine learning models using joblib.
