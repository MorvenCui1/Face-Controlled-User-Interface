# Face-Controlled-User-Interface
Uses facial landmarks of user as inputs to replace a keyboard and mouse, designed for users with impaired motor functions. Uses movement of nose to control cursor, left eye winking to left click, right eye winking to right click, and open mouth to bring up and down keyboard.
Uses MediaPipe and OpenCV to capture facial positions as inputs, uses PyAutoGUI to automate mouse and keyboard functions based on detected motions. Stores user facial landmarks data on SQLite database for future use in training machine learning models. Trains machine learning models using random forest with scikit-learn to adapt to facial landmarks of each user. Stores trained machine learning models using joblib.
